{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\") #local ==> to local 執行續\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkConf 用程式透過spark-submit 啟動時,會將設定值注入環境中,會在建立SparkConf物件時自動被執行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預設會檢查/conf/spark-defaults.conf設定檔並讀取其中的鍵,值對形式的配置值(以空白)做分割or --properties-file 指定其他的檔案位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if sparkConf pass to SaprkContext , and then it will can't be change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### almost of saprk can set it SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG -->Every RDD will point to father's RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 表示RDD的血統關係圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. to depend on user main() to build a RDD to point DAG\n",
    "2. action will let DAG to a execute \n",
    "3. cluster's' job will be arrange and execute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UI PAGE \n",
    "localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵效能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.平行度\n",
    "\n",
    "###應用程式分配到1000個核心,但是只執行30個任務,可提供平行度避免資源浪廢\n",
    "\n",
    "###2.序列化格式\n",
    "\n",
    "###Spark透過網路將資料存到池碟中,必須將物件序列化為二進位編碼格式。\n",
    "\n",
    "###3.記憶體管理\n",
    "\n",
    "###儲存RDD\n",
    "\n",
    "###資料洗牌與聚合操作的緩衝\n",
    "\n",
    "###使用者程式碼\n",
    "\n",
    "###硬體配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "儲存RDD 資料洗牌與聚合操作的緩衝 使用者程式碼 硬體配置"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
