{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 物件稱為DStream -離散物件->一定時間內到達的資料 ->,每個DStream 代表著每個時間刻度所到達的一串RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS\n",
    "Flume => Spark Stream(to build a DStream) => 轉換 或是 輸出\n",
    "Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.Duration\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.spark.SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#透過SparkConf建立StreamingContext ,並設定批次間隔為1秒\n",
    "val ssc = new StreamingContext(conf, Seconds(1))\n",
    "\n",
    "#local machine\n",
    "val lines = ssc.socketTextStream(\"localhost\", 7777)\n",
    "\n",
    "val errorLines = lines.filter(_.contains(\"error\"))\n",
    "\n",
    "errorLines.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start() #let Job give to SparkContext\n",
    "ssc.awaitTermination() #wait util the job is finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 串流Context只能使用被啟動一次,而且必須完成DStream與輸出操作後才能夠啟動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --class com.oreilly.lear..StreamingLogInput ASSEMBLY_JAR local[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc localhost 7777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkStreaming build a checkpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 週期性的資料放到可信賴的系統(HDFS or S3),if loss data will check from nearly checkpoint and cal again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DStreams => 非具態(stateless) and 具態(stateful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非具態(stateless):會獨立處理每個批次的資料,與先前的批次資料沒有相依關係 map() filter() reduceByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具態(stateful):使用先前批次中的資料或是中間結果來計算目前的批次資料並產出結果。會基於 slide windows or 時間變化的狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless: map(),flatMap(),filter(),repartition(),reduceByKey(),groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful:\n",
    "\n",
    "ssc.checkpoint(\"hdfs://...\")\n",
    "\n",
    "# windows\n",
    "val accessLogsWindow = accessLogsDStream.window(Seconds(30), Seconds(10))\n",
    "val windowCounts = accessLogsWindow.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 計算每個IP的訪問次數\n",
    "val ipDStreaming = accessLogsDStream.map(logEntry.getIpAddress(),1))\n",
    "val ipCountDStream = ipDStream.reduceByKeyAndWindow(\n",
    "{(x, y) => x +y}, //將進入視窗的新批次加入\n",
    "{(x, y) => x -y}, //將離開視窗的舊批次資料移除\n",
    "Seconds(30), //視窗長度\n",
    "Seconds(10)  //滑動長度\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### scala updateStateByKey() 對HTTP 回應碼計數\n",
    "def updateRunningSum(values: Seq[Long], state: Option[Long]) = {\n",
    "    Some(state.getOrElse(0L) + values.size)\n",
    "}\n",
    "val responseCodeStream = accessLogsDStream.map(log => (log.getResponseCode(),1L))\n",
    "val responseCodeCountDStream = responseCodeDStream.updateStateByKey(updateRunningSum _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipAddressRequestCount.saveAsTextFiles(\"outputDir\", \"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipAddressRequestCount.foreach{\n",
    "    rdd => rdd.foreachPartition {\n",
    "        //start to connect to DB\n",
    "        partition =>\n",
    "        partitionn.foreach {\n",
    "            item =>pass data to outside System\n",
    "        }\n",
    "        //close connect\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming 要從檔案建立串流時, 要求檔案的目錄名稱必須有一致的日期格式,必且是ACIDS的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val logData = ssc.textFileStream(logDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訂閱Panda 主題的客戶\n",
    "import org.apache.spark.streaming.kafka._\n",
    "...\n",
    "val topics = List((\"pandas\",1),(\"logs\",1)).toMap\n",
    "val topicLines = kafkaUtils.createStream(ssc. ZKQuorum, group, topics)\n",
    "topicLines.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取Panda主題\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import kafka.serializer.StringDecoder\n",
    "\n",
    "val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n",
    "\n",
    "val topicSet = List(\"pandas\", \"logs\").toSet\n",
    "val topicLines = kafkaUtils.createDirectStream[String, String,\n",
    "                StringDecoder, StringDecoder](ssc, kafkaParams, topicSet)\n",
    "StreamingLogInput.processLines(topicLines.map(_._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全天候運行Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. checkpoint\n",
    "2. 容錯議題\n",
    "3. unTrust input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
