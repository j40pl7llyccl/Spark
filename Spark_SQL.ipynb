{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.SparkSQL 對Python, Java和Scala語言提供了DataFrame框架,這類似關聯式資料庫的表格,可簡化處理結構化資料即時的工作\n",
    "### 2.SparkSQL can do the 結構化資料 的讀和寫\n",
    "### 3.Can let Users use SQL to query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SparkSQL is base on RDD where is call DataFrame. It is clude row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = sqlContext.read.json(\"/home/jordan/Desktop/testTweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## is very import to register table\n",
    "input.registerTempTable(\"testTweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topTweets = sqlContext \\\n",
    ".sql(\"\"\"select text, retweetCount from testTweets \\\n",
    "order by retweetCount limit 10\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|                text|retweetCount|\n",
      "+--------------------+------------+\n",
      "|Adventures With C...|           0|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topTweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = sqlContext.sql(\"\"\"select * from testTweets\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+---------------+------------------+-----------------+---------------+-----------+-------------------+-----------+-------------+------------+--------------------+--------------------+-----------+--------------------+-------------------+\n",
      "|contributorsIDs|           createdAt|currentUserRetweetId|hashtagEntities|                id|inReplyToStatusId|inReplyToUserId|isFavorited|isPossiblySensitive|isTruncated|mediaEntities|retweetCount|              source|                text|urlEntities|                user|userMentionEntities|\n",
      "+---------------+--------------------+--------------------+---------------+------------------+-----------------+---------------+-----------+-------------------+-----------+-------------+------------+--------------------+--------------------+-----------+--------------------+-------------------+\n",
      "|             []|Nov 4, 2014 4:56:...|                  -1|             []|529799371026485248|               -1|             -1|      false|              false|      false|           []|           0|<a href=\"http://t...|Adventures With C...|         []|[Aug 5, 2011 9:42...|                 []|\n",
      "+---------------+--------------------+--------------------+---------------+------------------+-----------------+---------------+-----------+-------------------+-----------+-------------+------------+--------------------+--------------------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              source|\n",
      "+--------------------+\n",
      "|<a href=\"http://t...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tweets.select(\"source\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.filter(df(\"age\") > 19)\n",
    "Tweets.groupBy(df(\"age\")).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[64] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topTweetRDD = topTweets.rdd.map(lambda row: row.text)\n",
    "topTweetRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache =>usually use to query same table was save in (in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLContext.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet is column-orient儲存格式,有效率的儲存在巢狀欄位結構,因此常用於Hadoop各類專案中,並且支援SparkSQL中所有類別型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Parquet\n",
    "rows = sqlContext.load(parquetFile, \"parquet\")\n",
    "names = rows.map(lambda row: row.name)\n",
    "print \"Everyone\"\n",
    "print names.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query Parquet\n",
    "tb1 = rows.registerTempTable(\"people\")\n",
    "pandaFriends = sqlContext.sql(\"select name from people where favotiteAnimal = \\\"panda\\\"\")\n",
    "print \"Panda friends\"\n",
    "print pandaFriends.map(lambda row: row.name).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sav Parquet\n",
    "pandaFriends.save\"hdfs://...\", \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sqlContext.read.json(\"/home/jordan/Desktop/testTweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "happyPeopleRDD = sc.parallelize([Row(name=\"holden\", favouriteBeverage=\"coffee\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "happyPeopleDataFrame = sqlContext.createDataFrame(happyPeopleRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[favouriteBeverage: string, name: string]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happyPeopleDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "happyPeopleDataFrame.registerTempTable(\"happy_people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDBC server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JDBC is base on sbin/start-thriftserver.sh\n",
    "is localhost:10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以透過Beeline客戶端連結至JDBC伺服器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./bin/beeline -u jdbc:hive2://localhost:1000\n",
    "            \n",
    "> show tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "hiveCtx.registerFunction(\"strLenPython\", lambda x: len(x), IntegerType())\n",
    "lengthDataFrame = hiveCtx.sql(\"select strLenPython('text) from tweets limit 10\")\n",
    "lengthDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "當資料被cache(),sparkSQL 使用 in-memory \n",
    "的column-based儲存方法,占用較少的記憶體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
