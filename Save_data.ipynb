{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read\n",
    "#read one file\n",
    "input = sc.textFile(\"READ.md\")\n",
    "\n",
    "# read more file 讀取多個檔案,並且帶入目錄路徑\n",
    "input = sc.textfile(\"~/Desktop\")\n",
    "\n",
    "#if file is small\n",
    "input = sc.wholeTextFiles() #it will return a pair RDD,and the file name is k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save\n",
    "\n",
    "#save the textFile\n",
    "result.saveAsTextFile(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Json file\n",
    "import json\n",
    "input = sc.texFile(\"Json file\")\n",
    "data = input.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Json\n",
    "data.filter(lambda x: 'lovesPandas' in x and x['lovesPandas']).map(lambda x:json.dumps(x)).saveAsTextFile(\"目錄\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果檔案沒有欄位有換行,textFile()\n",
    "import csv\n",
    "import StringIO\n",
    "\n",
    "def loadRecord(line):\n",
    "    \"\"\"parse one row Csv record\"\"\"\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input, filednames=[\"name\", \"favoriteAnimal\"])\n",
    "    return reader.next()\n",
    "\n",
    "input = sc.textFile(inputCSVfile).map(loadRecord)\n",
    "\n",
    "#如果檔案有欄位有換行\n",
    "def loadRecords(fileNameContents):\n",
    "    \"\"\"parse whole file record\"\"\"\n",
    "    input = StringIO.StringIO(fileNameContents[1])\n",
    "    reader = csv.DictReader(input, filednames=[\"name\", \"favoriteAnimal\"])\n",
    "    return reader\n",
    "\n",
    "fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save CSV file 建立一個映射保證輸出一致\n",
    "import csv\n",
    "import StringIO\n",
    "\n",
    "def writeRecords(records):\n",
    "    \"\"\"output file\"\"\"\n",
    "    output = StringIO.StringIO()\n",
    "    writer = csv.DictWriter(output, filednames=[\"name\", \"favoriteAnimal\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]\n",
    "\n",
    "pandaLovers = input.filter(lambda x:x['favoriteAnimal']=='panda')\n",
    "pandaLovers.mapPartitions(writeRecords).saveAsTextFile(\"outputfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.sequence(path, keyClass, valueClass, minPartition) do the hadoop Writeabl class to see\n",
    "#Read\n",
    "data = sc.sequenceFile(inFile, \"org.apache.io.Text\", \"org.apache.hadoop.io.IntWriteable\")\n",
    "#write\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark內建的輸入格式(textFile and SequenceFile )會自動處理某些壓縮格式,讀取壓縮檔時,壓縮編碼器會自動猜測壓縮的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from local file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必須這個檔案在所有節點上有相同的路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs://master:port/path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特過SparkSQL處理結構化資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. copy hive-site.xml to Spark ./conf/ => build a HiveContext Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. from pyspark.sql import HiveContext\n",
    "\n",
    "hiveCtx = HiveContext(sc)\n",
    "rows = hiveCtx.sql(\"Select * from users\")\n",
    "firstrow = rows.first()\n",
    "print firstrow.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = hiveCtx.jasonFile() => get row RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL => MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConnection() = {\n",
    "    class.forName(\"com.mysql.jdbc.Driver\").newInstance();\n",
    "    DriverManager.getConnection(\"jdbc://localhost/test?user=jordan\")\n",
    "}\n",
    "\n",
    "def extractValues(r: ResultSet) = {\n",
    "    (r.getInt(1), r.getString(2))\n",
    "}\n",
    "\n",
    "\n",
    "data = new jdbcRDD(sc, createConnection, \"select * from user where ?< = id and id <= ?\",\n",
    "                  lowerBound =1, upperBound=3, numPartition = 2, mapRow = extractValues)\n",
    "print (data.collect().toList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
